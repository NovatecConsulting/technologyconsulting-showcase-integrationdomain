cp-kafka-connect:
  # Default values for kafka-connect.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  replicaCount: 1

  ## Specify a imagePullPolicy
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  imagePullPolicy: IfNotPresent

  servicePort: 8083

  ## Kafka Connect properties
  ## ref: https://docs.confluent.io/current/connect/userguide.html#configuring-workers
  configurationOverrides:
    "bootstrap.servers": "confluent-platform-cp-kafka:9092" #"pkc-lq8gm.westeurope.azure.confluent.cloud:9092"
    "group.id": "connect-cluster"
    "internal.key.converter.schemas.enable": "false"
    "internal.value.converter.schemas.enable": "false"
    "offset.storage.topic": "connect-offsets"
    "offset.storage.partitions": "3"
    "config.storage.topic": "connect-configs"
    "status.storage.topic": "connect-status"
    "offset.flush.interval.ms": "10000"
    #"ssl.endpoint.identification.algorithm": "https"
    #"sasl.mechanism": "PLAIN"
    "request.timeout.ms": "20000"
    "retry.backoff.ms": "500"
    #"sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username='7RXI2RV7OJJ5F4EO' password='pxBOsJaZ1U1rMmDvp9vwZsRUByV1GSF7Ox+sJsDdmCOwmhnBEzNFBspy6SeIJ81q';"
    #"security.protocol": "SASL_SSL"
    #"consumer.ssl.endpoint.identification.algorithm": "https"
    #"consumer.sasl.mechanism": "PLAIN"
    "consumer.request.timeout.ms": "20000"
    "consumer.retry.backoff.ms": "500"
    #"consumer.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username='7RXI2RV7OJJ5F4EO' password='pxBOsJaZ1U1rMmDvp9vwZsRUByV1GSF7Ox+sJsDdmCOwmhnBEzNFBspy6SeIJ81q';"
    #"consumer.security.protocol": "SASL_SSL"
    #"producer.ssl.endpoint.identification.algorithm": "https"
    #"producer.sasl.mechanism": "PLAIN"
    "producer.request.timeout.ms": "20000"
    "producer.retry.backoff.ms": "500"
    #"producer.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username='7RXI2RV7OJJ5F4EO' password='pxBOsJaZ1U1rMmDvp9vwZsRUByV1GSF7Ox+sJsDdmCOwmhnBEzNFBspy6SeIJ81q';"
    #"producer.security.protocol": "SASL_SSL"
    "key.converter": "org.apache.kafka.connect.json.AvroConverter"
    "value.converter": "org.apache.kafka.connect.json.AvroConverter"
    "value.converter.schema.registry.url": "http://confluent-platform-cp-schema-registry:8081"
    "key.converter.schema.registry.url": "http://confluent-platform-cp-schema-registry:8081"
    "key.converter.schemas.enable": "true"
    "value.converter.schemas.enable": "true"
    "internal.key.converter": "org.apache.kafka.connect.json.JsonConverter"
    "internal.value.converter": "org.apache.kafka.connect.json.JsonConverter"
    "config.storage.replication.factor": "1"
    "offset.storage.replication.factor": "1"
    "status.storage.replication.factor": "1"
    "config.providers": "azure"
    "config.providers.azure.class": "io.lenses.connect.secrets.providers.AzureSecretProvider"
    "config.providers.azure.param.azure.auth.method": "credentials"
    "config.providers.azure.param.file.dir": "/connector-files/azure"
    "config.providers.azure.param.azure.client.id":
    "config.providers.azure.param.azure.secret.id":
    "config.providers.azure.param.azure.tenant.id":

  ## Kafka Connect JVM Heap Option
  heapOptions: "-Xms512M -Xmx512M"

  ## Additional env variables
  ## CUSTOM_SCRIPT_PATH is the path of the custom shell script to be ran mounted in a volume
  customEnv:
    #KAFKA_ZOOKEEPER_CONNECT: confluent-platform-zookeeper:2181
    #KAFKA_ADVERTISED_LISTENERS: INTERNAL://:9092,EXTERNAL://:9093
    #KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
    #KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
    #KAFKA_LISTENERS: INTERNAL://:9092,EXTERNAL://:9093
    # CUSTOM_SCRIPT_PATH: /etc/scripts/create-connectors.sh

  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
    #  cpu: 100m
    #  memory: 128Mi

  ## Custom pod annotations
  podAnnotations: {}

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  ## Taints to tolerate on node assignment:
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## Monitoring
  ## Kafka Connect JMX Settings
  ## ref: https://kafka.apache.org/documentation/#connect_monitoring
  jmx:
    port: 5555

  ## Prometheus Exporter Configuration
  ## ref: https://prometheus.io/docs/instrumenting/exporters/
  prometheus:
    ## JMX Exporter Configuration
    ## ref: https://github.com/prometheus/jmx_exporter
    jmx:
      enabled: true
      image: solsson/kafka-prometheus-jmx-exporter@sha256
      imageTag: 6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143
      imagePullPolicy: IfNotPresent
      port: 5556

      ## Resources configuration for the JMX exporter container.
      ## See the `resources` documentation above for details.
      resources: {}

  ## You can list load balanced service endpoint, or list of all brokers (which is hard in K8s).  e.g.:
  ## bootstrapServers: "PLAINTEXT://dozing-prawn-kafka-headless:9092"
  kafka:
    bootstrapServers: ""

  ## If the Kafka Chart is disabled a URL and port are required to connect
  ## e.g. gnoble-panther-cp-schema-registry:8081
  cp-schema-registry:
    url: ""

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  #volumeMounts:
  # - name: files
  #  mountPath: /etc/confluent/docker/ensure
  #   subPath: ensure

  ## List of volumeMounts for connect server container
  ## ref: https://kubernetes.io/docs/concepts/storage/volumes/
  #volumes:
  #  - name: conf


  ## Secret with multiple keys to serve the purpose of multiple secrets
  ## Values for all the keys will be base64 encoded when the Secret is created or updated
  ## ref: https://kubernetes.io/docs/concepts/configuration/secret/
  secrets:
    # username: kafka123
    # password: connect321

  ## These values are used only when "customEnv.CUSTOM_SCRIPT_PATH" is defined.
  ## "livenessProbe" is required only for the edge cases where the custom script to be ran takes too much time
  ## and errors by the ENTRYPOINT are ignored by the container
  ## As an example such a similar script is added to "cp-helm-charts/examples/create-connectors.sh"
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  livenessProbe:
    # httpGet:
    #   path: /connectors
    #   port: 8083
    # initialDelaySeconds: 30
    # periodSeconds: 5
    # failureThreshold: 10
