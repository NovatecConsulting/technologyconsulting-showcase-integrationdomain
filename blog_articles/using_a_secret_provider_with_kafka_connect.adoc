
= (WIP) Using a Secret Provider with Kafka Connect
== Teaser

Kafka Connect, coming along with many pre-build Connectors, can be a useful addition to your Kafka cluster. The setup can be done within minutes - until you come to the point where you start thinking about security. What is the best way to handle my secrets in Connector configurations when I - of course- do not want to expose them in plaintext? In this blog article, we step through an example scenario where it will be shown why and when to use Kafka Connect and how to inject credentials into Connector configurations. We will see that there are some pitfalls to be aware of if the Connect setup is not to take much more time than originally estimated.

== Why and when should you use Kafka Connect?

For better understanding, let's have a look at an example scenario: We assume that there is an existing system where one component is processing data that is needed by another component. This could be, for example, a domain that processes customer data and order information and writes it into a database. A webshop was later on added to our architecture and now needs the customer and order data. Of course, we could modify our order domain to pass the data directly to the webshop service. But first of all, in most cases, it will not be possible to modify existing systems. Second, we assume that there are plans to add additional web shop services, e.g. like Amazon or eBay, later on, that also need the same kind of data. To solve this, Kafka can serve as a central component from where all webshops will get their data from. Additionally, Kafka Connect makes it possible to get the customer and order data from the database into Kafka.

As shown in the below figure, the order domain, the database and Kafka Connect are running in Azure while the remaining part of the Kafka cluster is running in Confluent Cloud. Therefore, Kafka Connect needs to authenticate against the database and the Confluent Cloud cluster. The scenario was implemented as part of a showcase investigating how to use a Secret Provider for Kafka Connect. Feel free to have a look at the https://github.com/NovatecConsulting/technologyconsulting-showcase-integrationdomain[repository] and test the code by yourself!

image::architecture_overview.png[align=center]

== Kafka Connect and Confluent Cloud Authentication
Authentication of Connect against Confluent Cloud is the "easy" part of this scenario: The https://docs.confluent.io/cloud/current/cp-component/connect-cloud-config.html#standalone-cluster[documentation] describes how to edit the worker properties file to enable a connection. Note the properties consumer.sasl.jaas.config and producer.sasl.jaas.config where the values contain your Confluent Cloud API-key and API-secret. Those values need to be injected externally, while the best approach to do this depends on your setup and deployment process. Within the https://github.com/NovatecConsulting/technologyconsulting-showcase-integrationdomain[showcase repository], this was done using a Github Actions pipeline which pulls those secrets from the Azure key vault and injects them into the helm values.yaml file. This approach is also shortly discussed at the end of this article.
Connector Configuration and Secret Management

https://docs.confluent.io/platform/current/connect/index.html[Kafka Connect] is a framework that is using pre-build Connectors that enable to transfer data between sources and sinks and Kafka. There are https://www.confluent.io/hub/[about 200 official Connectors] that are available to access databases, key-value stores and file systems. A Kafka Connect Cluster consists of one or more Connectors of which the configurations are deployed via Kafka Connect's REST API.

image::connect_cluster.png[align=center]

The Connector's configuration will usually contain the credentials of the system you want to access, e.g., your database username and password:
PostgreSQL Sink Connector Configuration:

[source, json]
----
{
    "name": "postgres_sink",
    "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
        "tasks.max": 1,
        "topics": "example_topic",
        "connection.url": "jdbc:postgresql://example.postgres.database.azure.com:5432/db",
        "connection.user": "myDatabaseUser",
        "connection.password": "myDatabasePassword",
        "auto.create": "true",
        "table.name.format": "example_table",
        "dialect.name": "PostgreSqlDatabaseDialect",
        "db.timezone": "Europe/Berlin",
        "pk.mode": "record_key",
        "insert.mode": "upsert"
    }
}
----

When researching online and reading tutorials, you will notice that most examples will look similar to this. Confluent's documentation also provides https://docs.confluent.io/kafka-connect-jdbc/current/source-connector/index.html#load-the-jdbc-source-connector[example configurations], but most of them without using any credentials or using them in plaintext. Clearly, for examples and learning, this is sufficient but not when running in production. First, the Connectors configuration can be accessed and read using a simple curl-command - making it readable for everybody who has access to the REST API. Second, usernames and passwords are objects to change from time to time - and you do not want to edit your code and redeploy it every time. And last and most obviously: you do not want to expose the credentials in your code repository.
 +
To work around this, it possible to template the JSON configurations. Sensitive configuration properties can be passed as environment variables, e.g., via a Github Actions pipeline. But this approach would assume that all connectors are already known at deployment and still, the credentials would be accessible via the REST API. Despite that, an increasing number of connectors in your cluster will increase the number of environment variables.

As this must be a common issue, one could assume that there must be a better and easy solution for such an important topic. https://cwiki.apache.org/confluence/display/KAFKA/KIP-297%3A+Externalizing+Secrets+for+Connect+Configurations[KIP-297] addressed this question and introduced externalizing secrets provided by the interface ConfigProvider and the implementation FileConfigProvider. There are also some examples available as provided in this https://rmoff.net/2019/05/24/putting-kafka-connect-passwords-in-a-separate-file-/-externalising-secrets/[blog article]. This approach seemed to be an acceptable solution but still not ideal: When most of the system's components are running in Azure and are accessing secrets using the Azure Key Vault, how to enable that also the Connectors are accessing the secrets from there at _runtime_? This would lead to a single access point for secret management and would enable to add Connectors later on.

Lenses implemented the ConfigProvider interface as https://docs.lenses.io/3.1/connectors/secret-providers.html[Secret Provider], giving an answer for exactly this question. The Secret Provider can be added as a plugin to Kafka Connect's plugin path like the Connector JARs. In the following, we will use the Secret Provider in the context of the Azure Key Vault, but it can also be used with other cloud platforms and secret vaults.

== Adding the Secret Provider

First of all, before the Secret Provider is added, Kafka Connect needs to be configured to pick up the credentials from a ConfigProvider. To do this, the Kafka Connect worker configuration needs to be overwritten with some properties. Depending on your deployment process, this can be done by adding the properties as configurationOverrides within your helm values.yaml file. https://docs.lenses.io/3.1/connectors/secret-providers.html#configuration-options[Lenses' documentation] provides more details about this. Keep in mind that the Azure client id, secret id and tenant id are as well highly sensitive credentials and also need to be injected in some way. For example, it is possible to store those secrets as Github Secrets and inject them via a Github Actions pipeline into the values.yaml file. Again, this is no perfect solution since the credentials will be written as plaintext into the values.yaml file, but they are at least not exposed within your code.

To add the Secret Provider, we have to create a custom Docker image in which we add the Secret Provider to Connect's plugin path:

[source, dockerfile]
----
FROM confluentinc/cp-kafka-connect-base:5.5.1

ENV PLUGINS_DIR=/usr/share/confluent-hub-components/

RUN mkdir -p ${PLUGINS_DIR}/secret-provider \
&& wget -qP ${PLUGINS_DIR}/secret-provider https://github.com/lensesio/secret-provider/releases/download/2.1.6/secret-provider-2.1.6-all.jar
----

If it is left like that, Kafka Connect would later fail with numerous exceptions. Kafka Connect uses the CUP tool at startup, which tries to access a class from the added jar. Since the class is not included in CUB's classpath, the check performed by this tool will fail and the startup will cancel. Therefore, we need to add to the Dockerfile:
Dockerfile

[source, dockerfile]
----
ENV CUB_CLASSPATH='"/etc/confluent/docker/docker-utils.jar:/usr/share/java/kafka/*:/usr/share/confluent-hub-components/secret-provider/*"'
----

Testing this would still lead to exceptions: The current version of the Secret Provider and the newest version of Kafka are using conflicting Scala versions. To resolve this, there are only two options: Downgrading Kafka Connect to containing a suitable Scala version or updating the Secret Provider. If your Kafka cluster is already running and your deployment is dependent on a specific Kafka version, the first option will not be practicable for you. The latter option will be associated with much more effort. For simplicity, the Kafka version remains at 5.5.1 in this example.
Two issues fixed, but this still sounds too easy? Like Lenses also mentions in the plugin's documentation, the HttpClient-class will not be pickup up correctly if the Secret Provider is only located in the plugin's path. Therefore, it needs to be included within the classpath.
Dockerfile

[source, dockerfile]
----
ENV CONNECT_PLUGIN_PATH=/usr/share/java,/usr/share/confluent-hub-components/
ENV CLASSPATH=${PLUGINS_DIR}/secret-provider/*
----

This issue will be fixed as soon as the new release of the Azure SDK will be published. Of course, this workaround is no clean solution and comes along with additional constraints. Read more details about this topic in this blog article (LINK). But for now, the summarized Dockerfile looks like this:

[source, dockerfile]
----
FROM confluentinc/cp-kafka-connect-base:5.5.1

ENV PLUGINS_DIR=/usr/share/confluent-hub-components/

RUN mkdir -p ${PLUGINS_DIR}/secret-provider \
&& wget -qP ${PLUGINS_DIR}/secret-provider https://github.com/lensesio/secret-provider/releases/download/2.1.6/secret-provider-2.1.6-all.jar

ENV CONNECT_PLUGIN_PATH=/usr/share/java,/usr/share/confluent-hub-components/
ENV CLASSPATH=${PLUGINS_DIR}/secret-provider/*
ENV CUB_CLASSPATH='"/etc/confluent/docker/docker-utils.jar:/usr/share/java/kafka/*:/usr/share/confluent-hub-components/secret-provider/*"'

ARG CONNECTORS="confluentinc/kafka-connect-jdbc:10.0.1"
RUN for c in ${CONNECTORS}; do confluent-hub install --no-prompt $c; done;
----
It is now possible to adjust the Connector configurations so that the credentials will not be visible in plaintext anymore. Username and password are picked up at runtime from the Azure Key Vault and are inserted into the configurations file:
PostgreSQL Sink Connector Configuration:

[source, json]
----
{
"name": "postgres_sink",
    "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
        "tasks.max": 1,
        "topics": "example_topic",
    "connection.url": "jdbc:postgresql://example.postgres.database.azure.com:5432/db",
    "connection.user": "${azure:vault-example.vault.azure.net:my-database-user}",
    "connection.password": "${azure:vault-example.vault.azure.net:my-database-password}",
    "auto.create": "true",
    "table.name.format": "example_table",
    "dialect.name": "PostgreSqlDatabaseDialect",
    "db.timezone": "Europe/Berlin",
    "pk.mode": "record_key",
    "insert.mode": "upsert"
    }
}
----

== Open Questions and Improvements

Using the Kafka Connect Secret Provider comes along with some pitfalls. Those can be worked around as long as you are not using the latest Confluent version. The next release of the Azure SDK and an update of the Secret Provider will make the Secret Provider much easier to integrate into your project. +
As this summarized solution seems to be short and handy, not knowing all those details can cost a lot of time when working under time pressure and taking security into account. +
As already mentioned, injecting secrets into values.yaml files via a pipeline is no perfect approach. At least, it is an acceptable option for a first solution. In case of a Kubernetes deployment, the secrets will still be readable for everybody who can get the yaml-files of the pods. Instead, it could be considered to use Kubernetes secrets or to investigate other possibilities.

Feel free to leave a comment if you have any ideas or thoughts about this!
